{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80643f6b",
   "metadata": {},
   "source": [
    "# 01 — Data Collection\n",
    "\n",
    "In this notebook we demonstrate how to collect historical data for any ticker (**VIX**, **S&P 500**, **VIX Futures ETF**) from Yahoo Finance using Playwright and BeautifulSoup.\n",
    "\n",
    "**Outputs:**\n",
    "- Format: comma-separated files (csv)\n",
    "- `data/raw/vix_ytd_ohlc.csv`\n",
    "- `data/raw/sp500_ytd_ohlcv.csv`\n",
    "- `data/raw/vxx_ytd_ohlcv.csv`\n",
    "\n",
    "**Notes:** \n",
    "- Only educational use (respect Yahoo ToS).  \n",
    "- Dates in UTC; daily frequency.  \n",
    "- The scraping code (three script files) live in `/scripts/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37298a7",
   "metadata": {},
   "source": [
    "**Data collected**\n",
    "1. VIX: daily open, high, low, close\n",
    "2. S&P 500: daily open, high, low, close, volume\n",
    "3. VXX Futures ETF: daily open, high, low, close, volume\n",
    "\n",
    "**Default window**: Year-to-date YTD (Jan 02 2025 - Nov 06 2025 at the time of writing).\n",
    "\n",
    "Note that markets are closed on Jan 01."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d747e1d8",
   "metadata": {},
   "source": [
    "**Scraping runs as separate .py files (and not in the notebook)**\n",
    "\n",
    "The scrapers use Playwright to drive a real browser and parse Yahoo’s Historical Data table.\n",
    "\n",
    "Playwright has two APIs: a synchronous API and an asynchronous API.\n",
    "Jupyter notebooks already run an event loop under the hood. Mixing that loop with Playwright’s sync API can trigger errors like:\n",
    "“It looks like you are using Playwright Sync API inside the asyncio loop…”\n",
    "\n",
    "While there are workarounds (e.g., wrapping sync calls with `asyncio.to_thread(...)` or fully rewriting the scraper to async and using await), the simplest approach (which I took) is to:\n",
    "\n",
    "1. keep the scraper as a normal Python script, and\n",
    "\n",
    "2. run it from the terminal or open the files and run them individually, then\n",
    "\n",
    "3. load the produced CSVs in other notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e8dd54",
   "metadata": {},
   "source": [
    "**Customization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0a3d85",
   "metadata": {},
   "source": [
    "VIX (`scripts/vix.py`) currently uses YTD automatically via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c800cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "def ytd_epochs():\n",
    "    now = datetime.now(timezone.utc)\n",
    "    jan1 = datetime(now.year, 1, 1, tzinfo=timezone.utc)\n",
    "    return int(jan1.timestamp()), int(now.timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66694f08",
   "metadata": {},
   "source": [
    "and S&P 500 (`scripts/gspc.py`) currently uses a fixed link with period1/period2 embedded (VXX is similar):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff6d11ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp500_url():\n",
    "    # Use your provided link values\n",
    "    return (\n",
    "        \"https://finance.yahoo.com/quote/%5EGSPC/history/\"\n",
    "        \"?period1=1735746040&period2=1762443547\"\n",
    "        \"&interval=1d&filter=history&frequency=1d\"\n",
    "    )\n",
    "# Ticker symbol: ^GSPC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70dd853",
   "metadata": {},
   "source": [
    "To change the window, modify `history_url_for_vix()` or `sp500_url()` to compute your own period1/period2 Unix timestamps (UTC) and swap them in the URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ce7a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_url_for_vix():\n",
    "    p1, p2 = ytd_epochs()\n",
    "    # Daily history. (frequency=1d + interval=1d are both used by Yahoo)\n",
    "    return (\n",
    "        \"https://finance.yahoo.com/quote/%5EVIX/history/\"\n",
    "        f\"?period1={p1}&period2={p2}&interval=1d&filter=history&frequency=1d\"\n",
    "    )\n",
    "\n",
    "# Example: Jan 1, 2024 to Nov 6, 2025 (UTC)\n",
    "p1 = int(datetime(2024,1,1,tzinfo=timezone.utc).timestamp())\n",
    "p2 = int(datetime(2025,11,6,tzinfo=timezone.utc).timestamp())\n",
    "url = f\"...period1={p1}&period2={p2}&interval=1d&filter=history&frequency=1d\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e38337",
   "metadata": {},
   "source": [
    "**To change the frequency (daily/weekly/monthly)**\n",
    "\n",
    "Both URLs accept `interval=`:\n",
    "\n",
    "`interval=1d` → daily\n",
    "\n",
    "`interval=1wk` → weekly\n",
    "\n",
    "`interval=1mo` → monthly\n",
    "\n",
    "Just replace the interval query string in the URL builder inside the script and re-run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd672e",
   "metadata": {},
   "source": [
    "**To change stock/ticker**\n",
    "\n",
    "You can simply browse to the relevant Yahoo Finance page for historical data and copy the URL into the S&P 500 script to obtain data for any stock/ticker.\n",
    "\n",
    "You can also try simply changing the ticker symbol in the URL; though this may not always work.\n",
    "\n",
    "Other scripts provided inside `/scripts` have the following extensive functionality - these were used in an earlier project which required much larger amount of data. These output csv files into current directory.\n",
    "\n",
    "1. `scrape_oneStock`: Output weekly adj. close data for one particular stock/ticker symbol over the previous 2 years (current default: `AAPL`)\n",
    "\n",
    "2. `scrape_allStocks`: Output weekly adj. close data for any list of ticker symbols over the previous 2 years into one csv file. Requires a list \"sp500_tickers.csv\" to be saved in the current directory. Attempts for a maximum of 3 times per ticker symbol, and also saves a list of failed tickers as \"failed_tickers.csv\"\n",
    "\n",
    "3. `scrape_S&P_ordered`: Output list of individial ticker symbols in the S&P 500 ordered by market cap (descending order). This scrapes a particular website available online (*Slickcharts*) which maintains the list. The S&P Global used to provide a list, but does not do so presently (Nov 05 2025). For this script, I did not use Playwright but manually mimiced a Mozilla browser\n",
    "\n",
    "5. `combine_CSVs`: Combine two csv files into one (I used this routine to combine my initial scrape attempt and the second scrape on the failed ticker list to get one dataframe)\n",
    "\n",
    "Each of these routines can be customized to get more data (e.g. more than just adj. close for stocks, or daily data), or to scrape data from different sources/websites.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57b08d6",
   "metadata": {},
   "source": [
    "**Troubleshooting notes**\n",
    "\n",
    "Make sure to install Playwright (chromium) - see `environment.yml`\n",
    "\n",
    "Good internet speed is essential if a large amount of data is being scraped from Yahoo Finance (daily data for 500 stocks over two years took me about 30-60 minutes)\n",
    "\n",
    "Timeout waiting for table: network hiccups or consent banners can delay the table. Individual pop-ups (e.g. cookies) are not handled in these routines. Otherwise, you can increase the wait by inserting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b165c0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page.set_default_timeout(20000)  # 20 seconds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vix-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
